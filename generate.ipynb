{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import io\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from diffusers import DiffusionPipeline\n",
    "from datasets import load_dataset, Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:07<00:00,  1.07s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionPipeline {\n",
       "  \"_class_name\": \"StableDiffusionPipeline\",\n",
       "  \"_diffusers_version\": \"0.31.0\",\n",
       "  \"_name_or_path\": \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
       "  \"feature_extractor\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"requires_safety_checker\": true,\n",
       "  \"safety_checker\": [\n",
       "    \"stable_diffusion\",\n",
       "    \"StableDiffusionSafetyChecker\"\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"PNDMScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
    "pipeline.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the existing dataset from Hugging Face (or create a new one)\n",
    "dataset_name = \"DiegoP-S/DatasetSynthesis\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Bell_Pepper', 'Soybean', 'Apple', 'Pomegranate', 'Chilli_Pepper', 'Ginger', 'Watermelon', 'Cabbage', 'Sweet_Potato', 'Capsicum', 'Kiwi', 'Corn', 'Garlic', 'Potato', 'Beetroot', 'Jalapeno', 'Orange', 'Carrot', 'Pineapple', 'Sweetcorn', 'Tomato', 'Peas', 'Banana', 'Cauliflower', 'Mango', 'Paprika'])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing Onion_generated_image_0.png\n",
      "Doing Lemon_generated_image_0.png\n",
      "Doing Cucumber_generated_image_0.png\n",
      "Doing Spinach_generated_image_0.png\n",
      "Doing Jalapeno_generated_image_22.png\n",
      "Doing Sweet Potato_generated_image_0.png\n",
      "Doing Turnip_generated_image_0.png\n",
      "Doing Bell Pepper_generated_image_0.png\n",
      "Doing Chilli Pepper_generated_image_0.png\n",
      "Doing Eggplant_generated_image_0.png\n",
      "Doing Pear_generated_image_0.png\n",
      "Doing Grapes_generated_image_0.png\n"
     ]
    }
   ],
   "source": [
    "samples_per_class = 1\n",
    "batch_size = 1\n",
    "group = \"test/elephant\"\n",
    "# classes = {\"Apple\", \"Banana\", \"Beetroot\", \"Bell Pepper\", \"Cabbage\", \"Capsicum\", \"Carrot\", \"Cauliflower\", \"Chilli Pepper\", \"Corn\", \"Garlic\", \"Ginger\", \"Jalapeno\", \"Kiwi\", \"Mango\", \"Orange\", \"Paprika\", \"Peas\", \"Pineapple\", \"Pomegranate\", \"Potato\", \"Soybean\", \"Sweet Potato\", \"Sweetcorn\", \"Tomato\", \"Watermelon\", \"Pear\", \"Grapes\", \"Cucumber\", \"Onion\", \"Lemon\", \"Spinach\", \"Turnip\", \"Eggplant\"}\n",
    "classes = {\"Elephant\"}\n",
    "\n",
    "for class_name in classes:\n",
    "    split = {class_name: {\"image\": [], \"group\": [], \"file_name\": []}}\n",
    "    for i in range(samples_per_class):\n",
    "        file_name = f\"{class_name}_generated_image_{i}.png\"\n",
    "\n",
    "        # Check if image already in dataset\n",
    "        if class_name in dataset.keys() and file_name in dataset[class_name][\"file_name\"]:\n",
    "            continue\n",
    "\n",
    "        # Generate the image using the pipeline\n",
    "        print(class_name)\n",
    "        generated_image = pipeline(f\"{class_name}\").images[0]\n",
    "\n",
    "        # Convert the image to a binary format (e.g., PNG)\n",
    "        img_byte_arr = io.BytesIO()\n",
    "        generated_image.save(img_byte_arr, format='PNG')\n",
    "        img_byte_arr = img_byte_arr.getvalue()\n",
    "\n",
    "        # You can now upload the binary data to Hugging Face Dataset\n",
    "        split[class_name][\"file_name\"].append(file_name),\n",
    "        split[class_name][\"group\"].append(group),\n",
    "        split[class_name][\"image\"].append(img_byte_arr),\n",
    "\n",
    "        if i % batch_size == 1:\n",
    "            # Create a new dataset with the updated data\n",
    "            dataset_updated = DatasetDict({split_name: Dataset.from_dict(data) for split_name, data in split.items()})\n",
    "\n",
    "            # Push the updated dataset to Hugging Face\n",
    "            dataset_updated.push_to_hub(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 90.90ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset updated: https://huggingface.co/datasets/DiegoP-S/DatasetSynthesis\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataset with the updated data\n",
    "updated_dataset = Dataset.from_dict(existing_data)\n",
    "\n",
    "\n",
    "# Push the updated dataset to Hugging Face\n",
    "updated_dataset.push_to_hub(dataset_name)\n",
    "print(f\"Dataset updated: https://huggingface.co/datasets/{dataset_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
